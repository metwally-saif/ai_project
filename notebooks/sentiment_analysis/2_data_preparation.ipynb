{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "\n",
    "# Download NLTK resources if not already available\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Preview:\n",
      "                                                text  label\n",
      "0  $BYND - JPMorgan reels in expectations on Beyo...      0\n",
      "1  $CCL $RCL - Nomura points to bookings weakness...      0\n",
      "2  $CX - Cemex cut at Credit Suisse, J.P. Morgan ...      0\n",
      "3  $ESS: BTIG Research cuts to Neutral https://t....      0\n",
      "4  $FNKO - Funko slides after Piper Jaffray PT cu...      0\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11931 entries, 0 to 11930\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    11931 non-null  object\n",
      " 1   label   11931 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 186.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_dir = \"../../data/sentiment_analysis/raw/\"\n",
    "data_path = os.path.join(data_dir, \"sentiment_data.csv\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preview the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check dataset information\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text Example:\n",
      "                                                text  \\\n",
      "0  $BYND - JPMorgan reels in expectations on Beyo...   \n",
      "1  $CCL $RCL - Nomura points to bookings weakness...   \n",
      "2  $CX - Cemex cut at Credit Suisse, J.P. Morgan ...   \n",
      "3  $ESS: BTIG Research cuts to Neutral https://t....   \n",
      "4  $FNKO - Funko slides after Piper Jaffray PT cu...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0       bynd jpmorgan reels expectations beyond meat  \n",
      "1  ccl rcl nomura points bookings weakness carniv...  \n",
      "2  cx cemex cut credit suisse jp morgan weak buil...  \n",
      "3                     ess btig research cuts neutral  \n",
      "4             fnko funko slides piper jaffray pt cut  \n"
     ]
    }
   ],
   "source": [
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = word_tokenize(text)  # Tokenize text\n",
    "    text = [word for word in text if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(text)\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Preview cleaned text\n",
    "print(\"\\nCleaned Text Example:\")\n",
    "print(df[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "max_words = 10000  # Vocabulary size\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "# Convert text to sequences\n",
    "df['text_sequences'] = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "\n",
    "# Save tokenizer for future use\n",
    "tokenizer_path = os.path.join(\"../../data/sentiment_analysis/processed/\", \"tokenizer.json\")\n",
    "os.makedirs(os.path.dirname(tokenizer_path), exist_ok=True)\n",
    "with open(tokenizer_path, 'w') as f:\n",
    "    f.write(tokenizer.to_json())\n",
    "\n",
    "print(\"\\nTokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequence Example:\n",
      "                                        cleaned_text  \\\n",
      "0       bynd jpmorgan reels expectations beyond meat   \n",
      "1  ccl rcl nomura points bookings weakness carniv...   \n",
      "2  cx cemex cut credit suisse jp morgan weak buil...   \n",
      "3                     ess btig research cuts neutral   \n",
      "4             fnko funko slides piper jaffray pt cut   \n",
      "\n",
      "                                    padded_sequences  \n",
      "0  [2842, 717, 5568, 257, 686, 981, 0, 0, 0, 0, 0...  \n",
      "1  [4189, 4190, 1954, 117, 5569, 1119, 2843, 835,...  \n",
      "2  [8304, 8305, 68, 84, 517, 982, 270, 562, 1047,...  \n",
      "3  [5570, 1765, 503, 103, 464, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [8306, 2180, 1197, 1379, 2181, 1499, 68, 0, 0,...  \n"
     ]
    }
   ],
   "source": [
    "# Define maximum sequence length\n",
    "max_length = 100\n",
    "\n",
    "# Pad sequences\n",
    "df['padded_sequences'] = list(pad_sequences(df['text_sequences'], maxlen=max_length, padding='post'))\n",
    "\n",
    "# Verify padding\n",
    "print(\"\\nPadded Sequence Example:\")\n",
    "print(df[['cleaned_text', 'padded_sequences']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 9544 samples\n",
      "Validation set: 1193 samples\n",
      "Test set: 1194 samples\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and labels (y)\n",
    "X = np.array(list(df['padded_sequences']))\n",
    "y = df['label'].values\n",
    "\n",
    "# Split into training, validation, and test sets (80% / 10% / 10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Print split sizes\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save splits\n",
    "processed_dir = \"../../data/sentiment_analysis/processed/\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save as .npy for numpy arrays\n",
    "np.save(os.path.join(processed_dir, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(processed_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(processed_dir, \"X_val.npy\"), X_val)\n",
    "np.save(os.path.join(processed_dir, \"y_val.npy\"), y_val)\n",
    "np.save(os.path.join(processed_dir, \"X_test.npy\"), X_test)\n",
    "np.save(os.path.join(processed_dir, \"y_test.npy\"), y_test)\n",
    "\n",
    "print(\"\\nProcessed data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Preparation\n",
    "\n",
    "1. **Loaded and Cleaned Data:**\n",
    "   - The dataset contains `X` 11931 with three sentiment labels: Bearish, Bullish, and Neutral.\n",
    "   - Text was cleaned by removing URLs, special characters, and stopwords.\n",
    "\n",
    "2. **Tokenized and Padded Text:**\n",
    "   - Text was tokenized using a vocabulary of 10,000 words.\n",
    "   - Padded sequences were created with a fixed length of 100 tokens.\n",
    "\n",
    "3. **Split Data:**\n",
    "   - Training set: N 9544\n",
    "   - Validation set: M 1193 \n",
    "   - Test set: P 1194 \n",
    "\n",
    "4. **Saved Processed Data:**\n",
    "   - Tokenizer and datasets were saved for future use.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to `3_model_training.ipynb` to build and train neural network models for sentiment classification.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
